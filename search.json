[
  {
    "objectID": "forecast.html",
    "href": "forecast.html",
    "title": "FluSight ILI Sandbox Dashboard",
    "section": "",
    "text": "this is replaced"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "View The Latest Forecasts\n\nThis Dashboard is build with Quarto. To learn more about Quarto websites visit https://quarto.org/docs/websites.\nAuthors can use \\(\\LaTeX\\) to write equations and even put in callouts:\n\n\n\n\n\n\nNote\n\n\n\nLook at this note!"
  },
  {
    "objectID": "eval.html",
    "href": "eval.html",
    "title": "FluSight ILI Sandbox Dashboard",
    "section": "",
    "text": "this is replaced"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Accessing Hub Data",
    "section": "",
    "text": "To ensure greater access to the data created by and submitted to this hub, real-time copies of its model-output, target, and configuration files are hosted on the Hubverse’s Amazon Web Services (AWS) infrastructure, in a public S3 bucket: [hub-bucket-name]\nNote: For efficient storage, all model-output files in S3 are stored in parquet format, even if the original versions in the GitHub repository are .csv.\nGitHub remains the primary interface for operating the hub and collecting forecasts from modelers. However, the mirrors of hub files on S3 are the most convenient way to access hub data without using git/GitHub or cloning the entire hub to your local machine.\nThe sections below provide examples for accessing hub data on the cloud, depending on your goals and preferred tools. The options include:\n\n\n\n\n\n\n\nAccess Method\nDescription\n\n\n\n\nhubData (R)\nHubverse R client and R code for accessing hub data\n\n\nPolars (Python)\nPython open-source library for data manipulation\n\n\nAWS command line interface\nDownload hub data to your machine and use hubData or Polars for local access\n\n\n\nIn general, accessing the data directly from S3 (instead of downloading it first) is more convenient. However, if performance is critical (for example, you’re building an interactive visualization), or if you need to work offline, we recommend downloading the data first.\n\nhubData (R)Polars (Python)AWS CLI\n\n\nhubData, the Hubverse R client, can create an interactive session for accessing, filtering, and transforming hub model output data stored in S3.\nhubData is a good choice if you:\n\nalready use R for data analysis\nwant to interactively explore hub data from the cloud without downloading it\nwant to save a subset of the hub’s data (e.g., forecasts for a specific date or target) to your local machine\nwant to save hub data in a different file format (e.g., parquet to .csv)\n\n\n\nTo install hubData and its dependencies (including the dplyr and arrow packages), follow the instructions in the hubData documentation.\n\n\n\nhubData’s connect_hub() function returns an Arrow multi-file dataset that represents a hub’s model output data. The dataset can be filtered and transformed using dplyr and then materialized into a local data frame using the collect_hub() function.\n\n\n[hubData will be updated to access target data once the Hubverse target data standards are finalized.]\n\n\n\nBelow is an example of using hubData to connect to a hub on S3 and filter the model output data.\nlibrary(dplyr)\nlibrary(hubData)\n\nbucket_name &lt;- \"[hub-bucket-name]\"\nhub_bucket &lt;- s3_bucket(bucket_name)\nhub_con &lt;- hubData::connect_hub(hub_bucket, file_format = \"parquet\", skip_checks = TRUE)\nhub_con %&gt;%\n  dplyr::filter(location == \"MA\", output_type == \"quantile\") %&gt;%\n  hubData::collect_hub()\n\nfull hubData documentation\n\n\n\n\n\n\nThe Hubverse team is currently developing a Python client (hubDataPy). Until hubDataPy is ready, the Polars library is a good option for working with hub data in S3. Similar to pandas, Polars is based on dataframes and series. However, Polars has a more straightforward API and is designed to work with larger-than-memory datasets.\nPandas users can access hub data as described below and then use the to_pandas() method to convert a Polars dataframe to pandas format.\nPolars is a good choice if you:\n\nalready use Python for data analysis\nwant to interactively explore hub data from the cloud without downloading it\nwant to save a subset of the hub’s data (e.g., forecasts for a specific date or target) to your local machine\nwant to save hub data in a different file format (e.g., parquet to .csv)\n\n\n\nUse pip to install Polars:\npip install polars\n\n\n\nThe examples below use the Polars scan_parquet() function, which returns a LazyFrame. LazyFrames do not perform computations until necessary, so any filtering and transforms you apply to the data are deferred until an explicit collect() operation.\n\n\nGet all oracle-output files into a single DataFrame.\nimport polars as pl\n\noracle_data = pl.scan_parquet(\n    # the structure of the s3 link below will depend on how your hub organizes target data\n    \"s3://[hub-bucket-name]/target-data/oracle-output/*/*.parquet\",\n    storage_options={\"skip_signature\": \"true\"}\n)\n\n# filter and transform as needed and collect into a dataframe, for example:\noracle_dataframe = oracle_data.filter(pl.col(\"location\") == \"MA\").collect()\n\n\n\nGet the model-output files for a specific team (all rounds). This example uses glob patterns to read from data multiple files into a single dataset.\nimport polars as pl\n\nlf = pl.scan_parquet(\n    \"s3://[hub-bucket-name]/model-output/[modeling team name]/*.parquet\",\n    storage_options={\"skip_signature\": \"true\"}\n)\n\n\n\nIf your data uses hive-style partitioning, Polars can use the partitions to filter the data before reading it.\nfrom datetime import datetime\nimport polars as pl\n\noracle_data = pl.scan_parquet(\n    \"s3://[hub-bucket-name]/target-data/oracle-output/\",\n    hive_partitioning=True,\n    storage_options={\"skip_signature\": \"true\"}) \\\n.filter(pl.col(\"nowcast_date\") == datetime(2025, 2, 5)) \\\n.collect()\n\nFull documentation of the Polars Python API\n\n\n\n\n\n\nAWS provides a terminal-based command line interface (CLI) for exploring and downloading S3 files. This option is ideal if you:\n\nplan to work with hub data offline but don’t want to use git or GitHub\nwant to download a subset of the data (instead of the entire hub)\nare using the data for an application that requires local storage or fast response times\n\n\n\n\nInstall the AWS CLI using the instructions here\nYou can skip the instructions for setting up security credentials, since Hubverse data is public\n\n\n\n\nWhen using the AWS CLI, the --no-sign-request option is required, since it tells AWS to bypass a credential check (i.e., --no-sign-request allows anonymous access to public S3 data).\n\n\n\n\n\n\nNote\n\n\n\nFiles in the bucket’s raw directory should not be used for analysis (they’re for internal use only).\n\n\nList all directories in the hub’s S3 bucket:\naws s3 ls [hub-bucket-name] --no-sign-request\nList all files in the hub’s bucket:\naws s3 ls [hub-bucket-name] --recursive --no-sign-request\nDownload all of target-data contents to your current working directory:\naws s3 cp s3://[hub-bucket-name]/target-data/ . --recursive --no-sign-request\nDownload the model-output files for a specific team:\naws s3 cp s3://[hub-bucket-name]/[modeling-team-name]/UMass-flusion/ . --recursive --no-sign-request\n\nFull documentation for aws s3 ls\nFull documentation for aws s3 cp"
  },
  {
    "objectID": "data.html#accessing-hub-data-on-the-cloud",
    "href": "data.html#accessing-hub-data-on-the-cloud",
    "title": "Accessing Hub Data",
    "section": "",
    "text": "To ensure greater access to the data created by and submitted to this hub, real-time copies of its model-output, target, and configuration files are hosted on the Hubverse’s Amazon Web Services (AWS) infrastructure, in a public S3 bucket: [hub-bucket-name]\nNote: For efficient storage, all model-output files in S3 are stored in parquet format, even if the original versions in the GitHub repository are .csv.\nGitHub remains the primary interface for operating the hub and collecting forecasts from modelers. However, the mirrors of hub files on S3 are the most convenient way to access hub data without using git/GitHub or cloning the entire hub to your local machine.\nThe sections below provide examples for accessing hub data on the cloud, depending on your goals and preferred tools. The options include:\n\n\n\n\n\n\n\nAccess Method\nDescription\n\n\n\n\nhubData (R)\nHubverse R client and R code for accessing hub data\n\n\nPolars (Python)\nPython open-source library for data manipulation\n\n\nAWS command line interface\nDownload hub data to your machine and use hubData or Polars for local access\n\n\n\nIn general, accessing the data directly from S3 (instead of downloading it first) is more convenient. However, if performance is critical (for example, you’re building an interactive visualization), or if you need to work offline, we recommend downloading the data first.\n\nhubData (R)Polars (Python)AWS CLI\n\n\nhubData, the Hubverse R client, can create an interactive session for accessing, filtering, and transforming hub model output data stored in S3.\nhubData is a good choice if you:\n\nalready use R for data analysis\nwant to interactively explore hub data from the cloud without downloading it\nwant to save a subset of the hub’s data (e.g., forecasts for a specific date or target) to your local machine\nwant to save hub data in a different file format (e.g., parquet to .csv)\n\n\n\nTo install hubData and its dependencies (including the dplyr and arrow packages), follow the instructions in the hubData documentation.\n\n\n\nhubData’s connect_hub() function returns an Arrow multi-file dataset that represents a hub’s model output data. The dataset can be filtered and transformed using dplyr and then materialized into a local data frame using the collect_hub() function.\n\n\n[hubData will be updated to access target data once the Hubverse target data standards are finalized.]\n\n\n\nBelow is an example of using hubData to connect to a hub on S3 and filter the model output data.\nlibrary(dplyr)\nlibrary(hubData)\n\nbucket_name &lt;- \"[hub-bucket-name]\"\nhub_bucket &lt;- s3_bucket(bucket_name)\nhub_con &lt;- hubData::connect_hub(hub_bucket, file_format = \"parquet\", skip_checks = TRUE)\nhub_con %&gt;%\n  dplyr::filter(location == \"MA\", output_type == \"quantile\") %&gt;%\n  hubData::collect_hub()\n\nfull hubData documentation\n\n\n\n\n\n\nThe Hubverse team is currently developing a Python client (hubDataPy). Until hubDataPy is ready, the Polars library is a good option for working with hub data in S3. Similar to pandas, Polars is based on dataframes and series. However, Polars has a more straightforward API and is designed to work with larger-than-memory datasets.\nPandas users can access hub data as described below and then use the to_pandas() method to convert a Polars dataframe to pandas format.\nPolars is a good choice if you:\n\nalready use Python for data analysis\nwant to interactively explore hub data from the cloud without downloading it\nwant to save a subset of the hub’s data (e.g., forecasts for a specific date or target) to your local machine\nwant to save hub data in a different file format (e.g., parquet to .csv)\n\n\n\nUse pip to install Polars:\npip install polars\n\n\n\nThe examples below use the Polars scan_parquet() function, which returns a LazyFrame. LazyFrames do not perform computations until necessary, so any filtering and transforms you apply to the data are deferred until an explicit collect() operation.\n\n\nGet all oracle-output files into a single DataFrame.\nimport polars as pl\n\noracle_data = pl.scan_parquet(\n    # the structure of the s3 link below will depend on how your hub organizes target data\n    \"s3://[hub-bucket-name]/target-data/oracle-output/*/*.parquet\",\n    storage_options={\"skip_signature\": \"true\"}\n)\n\n# filter and transform as needed and collect into a dataframe, for example:\noracle_dataframe = oracle_data.filter(pl.col(\"location\") == \"MA\").collect()\n\n\n\nGet the model-output files for a specific team (all rounds). This example uses glob patterns to read from data multiple files into a single dataset.\nimport polars as pl\n\nlf = pl.scan_parquet(\n    \"s3://[hub-bucket-name]/model-output/[modeling team name]/*.parquet\",\n    storage_options={\"skip_signature\": \"true\"}\n)\n\n\n\nIf your data uses hive-style partitioning, Polars can use the partitions to filter the data before reading it.\nfrom datetime import datetime\nimport polars as pl\n\noracle_data = pl.scan_parquet(\n    \"s3://[hub-bucket-name]/target-data/oracle-output/\",\n    hive_partitioning=True,\n    storage_options={\"skip_signature\": \"true\"}) \\\n.filter(pl.col(\"nowcast_date\") == datetime(2025, 2, 5)) \\\n.collect()\n\nFull documentation of the Polars Python API\n\n\n\n\n\n\nAWS provides a terminal-based command line interface (CLI) for exploring and downloading S3 files. This option is ideal if you:\n\nplan to work with hub data offline but don’t want to use git or GitHub\nwant to download a subset of the data (instead of the entire hub)\nare using the data for an application that requires local storage or fast response times\n\n\n\n\nInstall the AWS CLI using the instructions here\nYou can skip the instructions for setting up security credentials, since Hubverse data is public\n\n\n\n\nWhen using the AWS CLI, the --no-sign-request option is required, since it tells AWS to bypass a credential check (i.e., --no-sign-request allows anonymous access to public S3 data).\n\n\n\n\n\n\nNote\n\n\n\nFiles in the bucket’s raw directory should not be used for analysis (they’re for internal use only).\n\n\nList all directories in the hub’s S3 bucket:\naws s3 ls [hub-bucket-name] --no-sign-request\nList all files in the hub’s bucket:\naws s3 ls [hub-bucket-name] --recursive --no-sign-request\nDownload all of target-data contents to your current working directory:\naws s3 cp s3://[hub-bucket-name]/target-data/ . --recursive --no-sign-request\nDownload the model-output files for a specific team:\naws s3 cp s3://[hub-bucket-name]/[modeling-team-name]/UMass-flusion/ . --recursive --no-sign-request\n\nFull documentation for aws s3 ls\nFull documentation for aws s3 cp"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the team",
    "section": "",
    "text": "This is our staff\n\n\n\n\n\nDale is the boss\n\n\n\n\n\n\n\n\nBubblegum is the scientist"
  },
  {
    "objectID": "about.html#introduction",
    "href": "about.html#introduction",
    "title": "About the team",
    "section": "",
    "text": "This is our staff\n\n\n\n\n\nDale is the boss\n\n\n\n\n\n\n\n\nBubblegum is the scientist"
  }
]